#### 你们的系统使用了哪种服务框架？为什么要这样技术选型？
| ———— | Spring Cloud | Dubbo | 
| :----- | :----- | :----- | 
| 并发性能 | 使用的是HTTP协议，性能与Dubbo对比稍微差点 | 是一款优秀的RPC框架，并发能力比Spring Cloud强 | 
| 注册中心 | 有全家桶配置中心: eureka nacos，亦可以选择Zookeeper | 一般选择Zookeeper | 
| 分布式配置中心 | nacos / Spring Cloud Config | 阿波罗 | 
| 网关 | Zuul / Spring Cloud Gateway  | 需引入其他网关组件 | 
| 负载均衡 | ribbon | 自带负载均衡 | 
| 熔断功能 | hystrix | 需引入其他熔断框架 | 
| 社区活跃度 | 活跃，版本更新快 | 不活跃 | 


#### 如果让你设计一个RPC框架，该从哪些方向考虑？
| 要点 | 解释 | 
| :----- | :----- | 
| <div style="width: 150px">服务发现与服务注册</div> | 1.如果我们想在Service A中调用Service B，那么我们首先得知道Service B的地址。所以，我们需要有一个服务注册中心，通过这个中心，服务可以把自己的信息注册进来，也可以获取到别的服务的信息；<br> 2.客户端也需要watch服务注册中心的目标服务的地址的变化 | 
| <div style="width: 150px">网络通信</div> | 1.服务和服务之间的网络通信模型， NIO/IO等等；<br> 2.客户端如何复用与服务端的连接， 而不是每次请求都重新创建一个新连接；<br> 3.客户端收到返回后，如何知道是哪个请求的返回并且做出正确处理 | 
| <div style="width: 150px">消息的序列化</div> | 服务间通信的消息通过什么方式进行序列化？Hessian，XML、JSON、Protobuf…，甚至Java原生的序列化方式，你总得选择一个 | 
| <div style="width: 150px">负载均衡</div> | 客户端通过服务注册中心拿到一堆地址，该调哪个呢？最简单的方式，可以通过RR、WRR的方式去做LB | 
| <div style="width: 150px">容灾</div> | 1.健康监测：在某一个服务节点挂掉的时候， 如何在服务注册中心删去这个服务地址？<br> 2.服务调用超时与重试：在调用一个服务实例的时候，如果超时或者报错，怎么处理？<br> 3.服务限流：如何限制最大并发数？这个又可以从客户端和服务端两个角度分析<br> | 


#### 能画一张图说说Spring Cloud的核心架构吗？它又是如何调用的？
![SpringCloud](/images/MicroService/SpringCloud.jpg)


* 首先每个服务启动的时候都需要往注册中心进行注册。
* 用户先对网关发起下单请求，网关收到请求后发现呃，是下单操作，要到订单系统，然后把请求路由到订单系统。
* 订单系统啪啦啪啦一顿操作，然后通过Feign去调用库存系统减库存，通知仓储服务发货，调用积分系统加积分。
* 在发起调用之前，订单系统还得通过Ribbon去注册中心去拉取各系统的注册表信息，并且挑一台机器给Feign来发起网络调用。


#### 你们的服务注册中心进行过选型调研吗？对比一Eureka和Zookeeper？
Zookeeper基于CP，不保证高可用，如果Zookeeper正在选主，或者Zookeeper集群中半数以上机器不可用，那么将无法获得数据。Eureka基于AP，能保证高可用，即使所有机器都挂了，也能拿到本地缓存的数据。作为注册中心，其实配置是不经常变动的，只有发版和机器出故障时会变。对于不经常变动的配置来说，CP是不合适的，而AP在遇到问题时可以用牺牲一致性来保证可用性，既返回旧数据，缓存数据。所以理论上Eureka是更适合作注册中心。而现实环境中大部分项目可能会使用Zookeeper，那是因为集群不够大，并且基本不会遇到用做注册中心的机器一半以上都挂了的情况。所以实际上也没什么大问题。
    

#### 你们对网关的技术选型是怎么考虑的？能对比一下各种网关的优劣吗？
* **动态路由**——新开发某个服务，动态把请求路径和服务的映射关系热加载到网关里去；服务增减机器，网关自动热感知。
* 灰度发布
* 授权认证
* **性能监控**——每个API接口的耗时、成功率、QPS。
* 系统日志
* 数据缓存
* 限流熔断


| ———— | Zuul1.x | Spring Cloud Gateway |
| :----- | :----- | :----- | 
| <div style="width: 200px">实现</div> | 基于Servlet2.x构建，使用阻塞的API | 基于Spring 5、Project Reactor、Spring Boot 2，使用非阻塞式的API |
| <div style="width: 200px">长连接</div> | 不支持 | 支持 |
| <div style="width: 200px">不适用场景</div> | 后端服务响应慢或者高并发场景下，因为线程数量是固定(有限)的，线程容易被耗尽，导致新请求被拒绝 | 中小流量的项目，使用Zuul1.x更合适 |
| <div style="width: 200px">限流</div> | 无 | 内置限流过滤器 |
| <div style="width: 200px">上手难度</div> | 同步编程，上手简单	| 门槛较高，上手难度中等 |
| <div style="width: 200px">Spring Cloud集成</div> | 是 | 是 |
| <div style="width: 200px">Sentinel集成</div> | 是 | 是 |
| <div style="width: 200px">技术栈沉淀</div> | Zuul1开源近七年，经受考验，稳定成熟 | 未见实际落地案例 |
| <div style="width: 200px">Github used by</div> | 1007 repositories | 102 repositories |
| <div style="width: 200px">Github issues</div> | 88 Open / 2736 Closed | 135 Open / 850 Closed |
| <div style="width: 200px">官方性能对比</div> | 2.09k / 12.56ms | 3.24k / 6.61ms |


Zuul1的开源时间很早，Netflix、Riot、携程、拍拍贷等公司都已经在生产环境中使用，自身经受了实践考验，是生产级的API网关产品。Gateway在2019年离开Spring Cloud孵化器，应用于生产的案例少，稳定性有待考证。从性能方面比较，两种产品在流量小的场景下性能表现差不多；并发高的场景下Gateway性能要好很多。从开发方面比较，Zuul1编程模型简单，易于扩展；Gateway编程模型稍难，代码阅读难度要比Zuul高不少，扩展也稍复杂一些。

    
#### 如果系统访问量比现在增加10倍,你们考虑过系统的扩容方案吗？
* **网关**——横向加机器
* **注册中心**:——纵向升配置
* **数据库**——纵向升配置


网关直接多部署10倍的机器即可，前面的Nginx做会负载均衡，把流量均匀分发给各个网关机器。服务扩容，都很简单的，多加机器，部署启动，自动注册到注册中心去。此时其他服务会自动感知到你的服务多加了一些机器。服务实例变多了10倍，此时几十个服务实例，几百个服务实例，对eureka机器会造成每秒几百请求，没问题，eureka机器，8核16G的配置，单机抗上千请求，很轻松。数据库本来是每秒几百请求，10倍，每秒高峰期是三四千请求，横向扩容很麻烦，此时可以考虑给单个数据库部署的机器提高配置，32核128G高配物理机，每秒钟抗几千请求问题不大。


#### 唯一ID生成机制中的snowflake算法的时钟回拨问题如何解决？
![snowflake](/images/MicroService/snowflake.jpg)


首先, snowflake的末尾12位是序列号, 用来记录同一毫秒内产生的不同id, 同一毫秒总共可以产生4096个id, 每一毫秒的序列号都是从0这个基础序列号开始递增假设我们的业务系统在单机上的QPS为3w/s, 那么其实平均每毫秒只需要产生30个id即可, 远没有达到设计的4096, 也就是说通常情况下序列号的使用都是处在一个低水位, 当发生时钟回拨的时候, 这些尚未被使用的序号就可以派上用场了。因此, 可以对给定的基础序列号稍加修改, 后面每发生一次时钟回拨就将基础序列号加上指定的步长, 例如开始时是从0递增, 发生一次时钟回拨后从1024开始递增, 再发生一次时钟回拨则从2048递增, 这样还能够满足3次的时钟回拨到同一时间点(发生这种操作就有点扯了)。


#### 我们一般到底用Zookeeper来干什么事儿？
* **配置管理**——在我们的应用中除了代码外，还有一些就是各种配置。比如数据库连接等。一般我们都是使用配置文件的方式，在代码中引入这些配置文件。但是当我们只有一种配置，只有一台服务器，并且不经常修改的时候，使用配置文件是一个很好的做法，但是如果我们配置非常多，有很多服务器都需要这个配置，而且还可能是动态的话使用配置文件就不是个好主意了。这个时候往往需要寻找一种集中管理配置的方法，我们在这个集中的地方修改了配置，所有对这个配置感兴趣的都可以获得变更。比如我们可以把配置放在数据库里，然后所有需要配置的服务都去这个数据库读取配置。但是，因为很多服务的正常运行都非常依赖这个配置，所以需要这个集中提供配置服务的服务具备很高的可靠性。一般我们可以用一个集群来提供这个配置服务，但是用集群提升可靠性，那如何保证配置在集群中的一致性呢？ 这个时候就需要使用一种实现了一致性协议的服务了。Zookeeper就是这种服务，它使用Zab这种一致性协议来提供一致性。现在有很多开源项目使用Zookeeper来维护配置，比如在HBase中，客户端就是连接一个Zookeeper，获得必要的HBase集群的配置信息，然后才可以进一步操作。还有在开源的消息队列Kafka中，也使用Zookeeper来维护broker的信息。在Alibaba开源的SOA框架Dubbo中也广泛的使用Zookeeper管理一些配置来实现服务治理。
* **名字服务**——名字服务这个就很好理解了。比如为了通过网络访问一个系统，我们得知道对方的IP地址，但是IP地址对人非常不友好，这个时候我们就需要使用域名来访问。但是计算机是不能是别域名的。怎么办呢？如果我们每台机器里都备有一份域名到IP地址的映射，这个倒是能解决一部分问题，但是如果域名对应的IP发生变化了又该怎么办呢？于是我们有了DNS这个东西。我们只需要访问一个大家熟知的(known)的点，它就会告诉你这个域名对应的IP是什么。在我们的应用中也会存在很多这类问题，特别是在我们的服务特别多的时候，如果我们在本地保存服务的地址的时候将非常不方便，但是如果我们只需要访问一个大家都熟知的访问点，这里提供统一的入口，那么维护起来将方便得多了。
* **分布式锁**——其实在第一篇文章中已经介绍了Zookeeper是一个分布式协调服务。这样我们就可以利用Zookeeper来协调多个分布式进程之间的活动。比如在一个分布式环境中，为了提高可靠性，我们的集群的每台服务器上都部署着同样的服务。但是，一件事情如果集群中的每个服务器都进行的话，那相互之间就要协调，编程起来将非常复杂。而如果我们只让一个服务进行操作，那又存在单点。通常还有一种做法就是使用分布式锁，在某个时刻只让一个服务去干活，当这台服务出问题的时候锁释放，立即fail over到另外的服务。这在很多分布式系统中都是这么做，这种设计有一个更好听的名字叫Leader Election(leader选举)。比如HBase的Master就是采用这种机制。但要注意的是分布式锁跟同一个进程的锁还是有区别的，所以使用的时候要比同一个进程里的锁更谨慎的使用。
* **集群管理**——在分布式的集群中，经常会由于各种原因，比如硬件故障，软件故障，网络问题，有些节点会进进出出。有新的节点加入进来，也有老的节点退出集群。这个时候，集群中其他机器需要感知到这种变化，然后根据这种变化做出对应的决策。比如我们是一个分布式存储系统，有一个中央控制节点负责存储的分配，当有新的存储进来的时候我们要根据现在集群目前的状态来分配存储节点。这个时候我们就需要动态感知到集群目前的状态。还有，比如一个分布式的SOA架构中，服务是一个集群提供的，当消费者访问某个服务时，就需要采用某种机制发现现在有哪些节点可以提供该服务(这也称之为服务发现，比如Alibaba开源的SOA框架Dubbo就采用了Zookeeper作为服务发现的底层机制)。还有开源的Kafka队列就采用了Zookeeper作为Consumer的上下线管理。


#### 为什么我们在分布式系统架构中需要使用Zookeeper集群？
* **单点问题**——单点问题是分布式环境中最常见也是最经典的问题之一，在很多分布式系统中都会存在这样的单点问题。具体地说，单点问题是指在一个分布式系统中，如果某一个组件出现故障就会引起整个系统的可用性大大下降甚至是处于瘫痪状态，那么我们就认为该组件存在单点问题。ZooKeeper 确实已经很好地解决了单点问题。我们已经了解到，基于“过半”设计原则，ZooKeeper 在运行期间，集群中至少有过半的机器保存了最新的数据。因此，只要集群中超过半数的机器还能够正常工作，整个集群就能够对外提供服务。
* **容灾**——在进行ZooKeeper的容灾方案设计过程中，我们要充分考虑到“过半原则”。也就是说，无论发生什么情况，我们必须保证ZooKeeper集群中有超过半数的机器能够正常工作。