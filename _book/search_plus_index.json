{"./":{"url":"./","title":"interview of sonin","keywords":"","body":"简介    "},"pages/SpringCloud/1266003503914942509.html":{"url":"pages/SpringCloud/1266003503914942509.html","title":"你们的系统使用了哪种服务框架?为什么要这样技术选型?","keywords":"","body":"Spring Cloud && Dubbo对比 区别 并发性能 注册中心 分布式配置中心 网关 负载均衡 熔断功能 社区活跃度 Spring 使用的是http协议,性能与dubbo对比稍微差点 有全家桶配置中心: eurake nacos,亦可以选择zookeeper nacos / spring cloud config zuul / srping cloud gateway ribbon hystrix 活跃,版本更新快 Dubbo 是一款优秀的RPC框架,并发能力比springcloud强 一般选择zookeeper 阿波罗 需引入其他网关组件 自带负载均衡 需引入其他熔断框架 不活跃 Tips 所以现在一般都会选择spring cloud全家桶做微服务，因为spring cloud胜在功能更全，有一些列可以开箱即用的组件，满足服务化后的各种场景需求。或者说dubbo就是一个纯正的RPC框架，对于服务之间远程调用，性能非常优秀，并发高，响应快，但是也仅仅是一个RPC框架，如果需要其他的功能，则需要引入其他的组件，因此在引入其他组件的过程中，可能会带来更多的问题。所以对于易用性这一块，spring cloud已经集成了各方面微服务所需要的组件，上手更快，拆坑更少，团队上手更容易，学习成本更低。可以开箱即用，快速上手。 "},"pages/SpringCloud/1266003504225320982.html":{"url":"pages/SpringCloud/1266003504225320982.html","title":"看过Dubbo源码吗?说说Dubbo的底层架构原理?","keywords":"","body":"简介 Dubbo是Alibaba开源的分布式服务框架，它最大的特点是按照分层的方式来架构，使用这种方式可以使各个层之间解耦合(或者最大限度地松耦合)。从服务模型的角度来看，Dubbo采用的是一种非常简单的模型，要么是提供方提供服务，要么是消费方消费服务，所以基于这一点可以抽象出服务提供方(Provider)和服务消费方(Consumer)两个角色。 是什么 简单说呢，Dubbo用起来就和EJB、WebService差不多，调用一个远程的服务(或者JavaBean)的时候在本地有一个接口，就像调用本地的方法一样去调用，它底层帮你实现好你的方法参数传输和远程服务运行结果传回之后的返回，就是RPC的一种封装啦~ 特点 它主要是使用高效的网络框架和序列化框架，让分布式服务之间调用效率更高。 采用注册中心管理众多的服务接口地址，当你想调用服务的时候只需要跟注册中心询问即可，不用像使用WebService一样每个服务都得记录好接口调用方式。注册中心主要就是负责dubbo的所有服务地址列表维护，并且可以通过在ZooKeeper节点中设置相应的值来实现对这个服务的权重、优先级、是否可用、路由、权限等的控制。之后在Dubbo的管理控制台对服务的一堆治理策略设置和调整，实际上就是修改了注册中心中的服务对应的配置数据(即修改了zookeeper中服务对应的节点的配置数据)。之后Consumer从注册中心请求到服务的数据时就能根据这些配置数据进行相应的治理配置参数的代码执行生效。 监控中心实现对服务方和调用方之间运行状态的监控，还能控制服务的优先级、权限、权重、上下线等，让整个庞大的分布式服务系统的维护和治理比较方便。 高可用有个服务宕机了?注册中心就会从服务列表去掉该节点。还是调用到了?客户端会向注册中心请求另一台可用的服务节点重新调用。注册中心宕机?注册中心也能实现高可用(ZooKeeper)。 负载均衡：采用软负载均衡算法实现对多个相同服务的节点的请求负载均衡。 RPC之Dubbo实现 主要为三点，动态代理、反射、socket网络编程 客户端使用动态代理的方式，“假装”实现了createOrder方法。 方法相关的数据通过序列化，进入到socket服务器。dubbo的socket实现为Netty。 服务端从socket服务器取出数据，通过反射的方式找到“真实”的服务实现。 服务端的方法在服务启动时已注入。 服务发现层，可用zookeeper。zookeeper保证了CP(一致性，分区容错性)。缺点：master节点挂掉时，需要时间重新选择master，这段时间内注册中心将不可用。 注意：服务端可消费端注册成功后，通讯只走socket服务器，不会经过注册中心。 核心技术简介 客户端发起接口调用； 服务中间件进行路由选址：找到具体接口实现的服务地址； 客户端将请求信息进行编码(序列化: 方法名，接口名，参数，版本号等)； 建立与服务端的通讯(不是调度中心，而是客户端与服务端直连)； 服务端将接收到的信息进行反编码(反序列化)； 根据信息找到服务端的接口实现类； 将执行结果反馈给客户端。 "},"pages/SpringCloud/1266003504359538773.html":{"url":"pages/SpringCloud/1266003504359538773.html","title":"咱们来聊点深入的,说说Dubbo底层的网络通信机制原理?","keywords":"","body":"基本信息 连接个数: 单连接 连接方式: 长连接 传输协议: TCP 传输方式: NIO异步传输 序列化: Hessian二进制序列化 适用范围： 传入传出参数数据包较小（建议小于100K），消费者比提供者个数多，单一消费者无法压满提供者，尽量不要用dubbo协议传输大文件或超大字符串。 适用场景: 常规远程服务方法调用 同步远程调用 客户端线程调用远程接口，向服务端发送请求，同时当前线程应该处于“暂停”状态，即线程不能向后执行了，必需要拿到服务端给自己的结果后才能向后执行； 服务端接到客户端请求后，处理请求，将结果给客户端； 客户端收到结果，然后当前线程继续往后执行； 基本原理 client一个线程调用远程接口，生成一个唯一的ID（比如一段随机字符串，UUID等），Dubbo是使用AtomicLong从0开始累计数字的； 将打包的方法调用信息（如调用的接口名称，方法名称，参数值列表等），和处理结果的回调对象callback，全部封装在一起，组成一个对象object； 向专门存放调用信息的全局ConcurrentHashMap里面put(ID, object)； 将ID和打包的方法调用信息封装成一对象connRequest，使用IoSession.write(connRequest)异步发送出去； 当前线程再使用callback的get()方法试图获取远程返回的结果，在get()内部，则使用synchronized获取回调对象callback的锁， 再先检测是否已经获取到结果，如果没有，然后调用callback的wait()方法，释放callback上的锁，让当前线程处于等待状态； 服务端接收到请求并处理后，将结果（此结果中包含了前面的ID，即回传）发送给客户端，客户端socket连接上专门监听消息的线程收到消息，分析结果，取到ID，再从前面的ConcurrentHashMap里面get(ID)，从而找到callback，将方法调用结果设置到callback对象里； 监听线程接着使用synchronized获取回调对象callback的锁（因为前面调用过wait()，那个线程已释放callback的锁了），再notifyAll()，唤醒前面处于等待状态的线程继续执行（callback的get()方法继续执行就能拿到调用结果了,这里的callback对象是每次调用产生一个新的，不能共享，否则会有问题；另外ID必需至少保证在一个Socket连接里面是唯一的。），至此，整个过程结束。 当前线程怎么让它\"暂停\",等结果回来后,再向后执行? 答: 先生成一个对象obj，在一个全局map里put(ID,obj)存放起来，再用synchronized获取obj锁，再调用obj.wait()让当前线程处于等待状态，然后另一消息监听线程等到服务端结果来了后，再map.get(ID)找到obj，再用synchronized获取obj锁，再调用obj.notifyAll()唤醒前面处于等待状态的线程。 正如前面所说,Socket通信是一个全双工的方式,如果有多个线程同时进行远程方法调用,这时建立在client server之间的socket连接上会有很多双方发送的消息传递,前后顺序也可能是乱七八糟的,server处理完结果后,将结果消息发送给client,client收到很多消息,怎么知道哪个消息结果是原先哪个线程调用的? 答: 使用一个ID，让其唯一，然后传递给服务端，再服务端又回传回来，这样就知道结果是原先哪个线程的了。 "},"pages/SpringCloud/1266003504476979219.html":{"url":"pages/SpringCloud/1266003504476979219.html","title":"Dubbo框架从架构设计角度,是怎么保证极高的可扩展性?","keywords":"","body":"Dubbo SPI特点 对Dubbo进行扩展，不需要改动Dubbo的源码。 自定义的Dubbo的扩展点实现，是一个普通的Java类，Dubbo没有引入任何Dubbo特有的元素，对代码侵入性几乎为零。 将扩展注册到Dubbo中，只需要在ClassPath中添加配置文件。使用简单。而且不会对现有代码造成影响。符合开闭原则。 Dubbo的扩展机制支持IoC,AoP等高级功能。 Dubbo的扩展机制能很好的支持第三方IoC容器，默认支持Spring Bean，可自己扩展来支持其他容器，比如Google的Guice。 切换扩展点的实现，只需要在配置文件中修改具体的实现，不需要改代码。使用方便。 Dubbo底层架构原理图 "},"pages/SpringCloud/1266003504686694408.html":{"url":"pages/SpringCloud/1266003504686694408.html","title":"如果让你设计一个RPC框架,网络通信 代理机制 负载均衡?","keywords":"","body":"什么是RPC RPC的全称是Remote Procedure Call，远程过程调用。RPC框架有很多，比如hsf、dubbo等等。借助RPC框架，我们在写业务代码的时候可以不需要去考虑服务之间的通信等问题，在调用远程服务的时候就像调用本地的方法那么简单。 组成部分 简化本地调用流程既然我们要像调用本地方法那样调用远程服务， 那么就应该生成代理来隐藏调用远程服务的细节。 这些细节包括但不限于以下所列出的关注点。 服务发现与服务注册 如果我们想在Service A中调用Service B，那么我们首先得知道Service B的地址。 所以，我们需要有一个服务注册中心，通过这个中心，服务可以把自己的信息注册进来，也可以获取到别的服务的信息。 客户端也需要watch服务注册中心的目标服务的地址的变化。 网络通信 服务和服务之间的网络通信模型， NIO/IO等等 客户端如何复用与服务端的连接， 而不是每次请求都重新创建一个新连接？ 客户端收到返回后，如何知道是哪个请求的返回并且做出正确处理？ 消息的序列化服务间通信的消息通过什么方式进行序列化？ hessian，XML、JSON、Protobuf、……, 甚至Java原生的序列化方式， 你总得选择一个。 负载均衡 客户端通过服务注册中心拿到一堆地址，该调哪个呢？最简单的方式，可以通过RR、WRR的方式去做LB。 根据服务实例的metrics做出动态调整, 比如响应时间等 利用一致性哈希， 提高本地缓存利用率 容灾 健康监测: 在某一个服务节点挂掉的时候， 如何在服务注册中心删去这个服务地址？ 服务调用超时与重试: 在调用一个服务实例的时候，如果超时或者报错，怎么处理？ 服务限流: 如何限制最大并发数？这个又可以从客户端和服务端两个角度分析。 "},"pages/SpringCloud/1266003504770580518.html":{"url":"pages/SpringCloud/1266003504770580518.html","title":"平时除了使用外,有研究过Spring Cloud的底层架构原理吗?","keywords":"","body":"SpringCloud架构原理图 Spring Cloud核心组件 Eureka各个服务启动时，Eureka Client都会将服务注册到Eureka Server，并且Eureka Client还可以反过来从Eureka Server拉取注册表，从而知道其他服务在哪里 Ribbon服务间发起请求的时候，基于Ribbon做负载均衡，从一个服务的多台机器中选择一台 Feign基于Feign的动态代理机制，根据注解和选择的机器，拼接请求URL地址，发起请求 Hystrix发起请求是通过Hystrix的线程池来走的，不同的服务走不同的线程池，实现了不同服务调用的隔离，避免了服务雪崩的问题 Zuul如果前端、移动端要调用后端系统，统一从Zuul网关进入，由Zuul网关转发请求给对应的服务 "},"pages/SpringCloud/1266003505043210273.html":{"url":"pages/SpringCloud/1266003505043210273.html","title":"从底层实现原理的角度,对比一下Dubbo和Spring Cloud的区别?","keywords":"","body":"Dubbo && SpringCloud原理图 RPC && SpringCloud原理图 相同点 都需要 服务提供方，注册中心，服务消费方。 Dubbo Provider: 暴露服务的提供方，可以通过jar或者容器的方式启动服务。 Consumer: 调用远程服务的服务消费方。 Registry: 服务注册中心和发现中心。 Monitor: 统计服务和调用次数，调用时间监控中心。(dubbo的控制台页面中可以显示，目前只有一个简单版本) Container: 服务运行的容器。 Spring Cloud Service Provider: 暴露服务的提供方。 Service Consumer: 调用远程服务的服务消费方。 EureKa Server: 服务注册中心和服务发现中心。 比较 从核心要素来看 Spring Cloud 更胜一筹，在开发过程中只要整合Spring Cloud的子项目就可以顺利的完成各种组件的融合，而Dubbo缺需要通过实现各种Filter来做定制，开发成本以及技术难度略高。Dubbo只是实现了服务治理，而Spring Cloud子项目分别覆盖了微服务架构下的众多部件，而服务治理只是其中的一个方面。Dubbo提供了各种Filter，对于上述中“无”的要素，可以通过扩展Filter来完善。 分布式配置：可以使用淘宝的diamond、百度的disconf来实现分布式配置管理 服务跟踪：可以使用京东开源的Hydra，或者扩展Filter用Zippin来做服务跟踪 批量任务：可以使用当当开源的Elastic-Job、tbschedule 从协议上看Dubbo缺省协议采用单一长连接和NIO异步通讯，适合于小数据量大并发的服务调用，以及服务消费者机器数远大于服务提供者机器数的情况，二进制的传输，占用带宽会更少。Spring Cloud 使用HTTP协议的REST APIdubbo支持各种通信协议，而且消费方和服务方使用长链接方式交互，http协议传输，消耗带宽会比较多，同时使用http协议一般会使用JSON报文，消耗会更大。通信速度上Dubbo略胜Spring Cloud，如果对于系统的响应时间有严格要求，长链接更合适。 从服务依赖方式看Dubbo服务依赖略重，需要有完善的版本管理机制，但是程序入侵少。而Spring Cloud通过Json交互，省略了版本管理的问题，但是具体字段含义需要统一管理，自身Rest API方式交互，为跨平台调用奠定了基础。 从组件运行流程看Dubbo每个组件都是需要部署在单独的服务器上，gateway用来接受前端请求、聚合服务，并批量调用后台原子服务。每个service层和单独的DB交互。 SpringCloud所有请求都统一通过 API 网关(Zuul)来访问内部服务。网关接收到请求后，从注册中心（Eureka）获取可用服务。由Ribbon进行均衡负载后，分发到后端的具体实例。微服务之间通过 Feign 进行通信处理业务。业务部署方式相同，都需要前置一个网关来隔绝外部直接调用原子服务的风险。Dubbo需要自己开发一套API 网关，而Spring Cloud则可以通过Zuul配置即可完成网关定制。使用方式上Spring Cloud略胜一筹。 "},"pages/SpringCloud/1266003505391337531.html":{"url":"pages/SpringCloud/1266003505391337531.html","title":"你们的服务注册中心进行过选型调研吗?对比一Eureka和Zookeeper?","keywords":"","body":"Eureka && Zookeeper Eureka (AP)   peer-to-peer，部署一个集群，但是集群里每个机器的地位是对等的，各个服务可以向任何一个Eureka实例服务注册和服务发现，集群里任何一个Eureka实例接收到写请求之后，会自动同步给其他所有的Eureka实例。Eureka是peer模式，可能还没同步数据过去，结果自己就死了，此时还是可以继续从别的机器上拉取注册表，但是看到的就不是最新的数据了，但是保证了可用性，强一致，最终一致性。 单点问题eureka需要部署服务，服务自身需要做集群，增加了系统部署的复杂性。 数据同步各服务之间数据同步是异步的，定时的，这会导致节点间一定时间内，数据不一致；并且，在数据复制的过程中，如果持有新实例注册信息的注册中心自身挂掉了，这个实例就无法得到注册； 自我保护机制注册中心自身如果监测到某个实例的心跳成功比例一定时间内小于一定的阈值，这个实例注册信息会被保护起来，不会注销掉，等到这个心跳成功比例大于阈值时，退出自我保护机制。在这个保护期内，如果服务挂了，那这个实例信息其实时有问题的，应该被剔除。 心跳压力如果注册中心注册的实例过多，比如500个，每个间隔30s发出一次续约心跳，那30s内，就是15000个心跳连接，这个心跳的请求可能大于实际业务发出的请求。 健康检查机制健康检查比较单一，仅仅检查心跳是不够的，心跳还在，说明服务进程没死，那服务所在的硬件问题如内存满载，关联的db挂了等，这些都无法得到反应，所以服务可能并不能提供服务了，但是服务还在注册中心的列表中。 维护风险官方宣布2.0的开源工作停止了，继续使用的责任自负。 Zookeeper (CP)   Leader + Follower两种角色，只有Leader可以负责写也就是服务注册，他可以把数据同步给Follower，读的时候leader/follower都可以读。ZooKeeper是有一个leader节点会接收数据， 然后同步写其他节点，一旦leader挂了，要重新选举leader，这个过程里为了保证C，就牺牲了A，不可用一段时间，但是一个leader选举好了，那么就可以继续写数据了，保证一致性。 重java开发，引入依赖多，对于服务器而言太重，部署复杂，不支持多数据中心。对服务侵入大。 健康检查检查方式单一，需要消费者自己实现，也是靠心跳连接保活，连接断开，就是服务挂了，服务就会被剔除。 更新非常稳定，更新少，微服务架构下，对于做专业的注册中心而言，功能匮乏，丧失了快速迭代的能力，不够与时俱进，不够灵活。 算法paxos算法，复杂难懂。 etcd 未使用过，资料了解，其本质上是一个比zk轻量的分布式键值对存储系统，但是需要搭配其他小工具才能较好较易用的实现注册中心功能。但是，为了实现A功能，又额外引入了B和C工具，不是一个优雅的实现方案，而且不支持多数据中心,无web管理页面。 consul (CP) 数据一致性 raft算法，实现思路从源头上避免了数据不一致性。注册时，超过半数没有拿到信息，那就注册失败。 开箱即用 集成简单，不依赖其他工具，使用也简单，支持2种服务注册方式：配置文件，http api。 kv存储 支持和zk和etcd一样的kv存储，可做配置中心。 健康检查 健康检查支持好，提供多种健康检查功能，比如服务返回的状态码，内存利用率等。 心跳 服务状态的检查，不是直接向注册中心发心跳，而是agent向服务发出健康监测。 web管理页面 官方提供良好的web管理页面。 活跃 社区很活跃，更新频繁。 Nacos 这个最近也挺火，待了解。 Tips 一致性保障: CP or AP CAP: C是一致性，A是可用性，P是分区容错性。 ZooKeeper是有一个leader节点会接收数据， 然后同步写其他节点，一旦leader挂了，要重新选举leader，这个过程里为了保证C，就牺牲了A，不可用一段时间，但是一个leader选举好了，那么就可以继续写数据了，保证一致性。Eureka是peer模式，可能还没同步数据过去，结果自己就死了，此时还是可以继续从别的机器上拉取注册表，但是看到的就不是最新的数据了，但是保证了可用性，强一致，最终一致性。 "},"pages/SpringCloud/1266003505634607158.html":{"url":"pages/SpringCloud/1266003505634607158.html","title":"画图阐述一下你们的服务注册中心部署架构,生产环境下怎么保证高可用?","keywords":"","body":"图略 高可用 Eureka高可用，至少2台做集群。两个分支内相互配置另一台的ip和端口号。 "},"pages/SpringCloud/1266003505915625504.html":{"url":"pages/SpringCloud/1266003505915625504.html","title":"你们系统遇到过服务发现过慢的问题吗?怎么优化和解决的?","keywords":"","body":"客户端的有效负载缓存应该更新的时间间隔,默认为30 * 1000ms eureka.server.responseCacheUpdateIntervalMs = 30000(30s) -> 3000(3s) 从eureka服务器注册表中获取注册信息的时间间隔(s),默认为30s eureka.client.registryFetchIntervalSeconds = 30000 -> 3000 客户端多长时间发送心跳给eureka服务器,表明它仍然活着,默认为30s eureka.client.leaseRenewalIntervalInSeconds = 30 -> 3 过期实例应该剔除的时间间隔,单位为毫秒,默认为60 * 1000 eureka.server.evictionIntervalTimerInMs = 60000 -> 6000(6s) Eureka服务器在接收到实例的最后一次发出的心跳后,需要等待多久才可以将此实例删除,默认为90s eureka.instance.leaseExpirationDurationInSeconds = 90 -> 9(s) 服务发现的时效性变成秒级,几秒钟可以感知服务的上线和下线 "},"pages/SpringCloud/1266003506066620430.html":{"url":"pages/SpringCloud/1266003506066620430.html","title":"说一下自己公司的服务注册中心怎么技术选型的?生产环境中应该怎么优化?","keywords":"","body":"服务注册,故障和发现的时效性是多长时间?注册中心最大能支撑多少服务实例? 如何部署的，几台机器，每台机器的配置如何，会用比较高配置的机器来做，8核16G，16核32G的高配置机器来搞，基本上可以做到每台机器每秒钟的请求支撑几千绝对没问题。 可用性如何来保证? 有没有做过一些优化，服务注册、故障以及发现的时效性，是否可以优化一下，用eureka的话，可以尝试一下，配合我们讲解的那些参数，优化一下时效性，服务上线、故障到发现是几秒钟的时效性。zk，一旦服务挂掉，zk感知到以及通知其他服务的时效性，服务注册到zk之后通知到其他服务的时效性，leader挂掉之后可用性是否会出现短暂的问题，为了去换取一致性。 "},"pages/SpringCloud/1266003506205032504.html":{"url":"pages/SpringCloud/1266003506205032504.html","title":"你们对网关的技术选型是怎么考虑的?能对比一下各种网关的优劣吗?","keywords":"","body":"网关的核心功能 动态路由：新开发某个服务，动态把请求路径和服务的映射关系热加载到网关里去；服务增减机器，网关自动热感知 灰度发布 授权认证 性能监控：每个API接口的耗时、成功率、QPS 系统日志 数据缓存 限流熔断 Zuul && Spring Cloud Gateway zuul是Netflix的产品，gateway是spring全家桶的亲儿子。zuul 更新维护不积极，所以gateway自己做了网关，就是为了替代zuul zuul 1.0和 2.0差别很大，1.0版本是基于servlet的同步阻塞io，2.0是基于netty通信的异步io，并发能力2.0版本大大提升。但是2.0文档相对不友好。gateway本身是基于netty通信的异步io，并发能力很强。 gateway文档齐全，架构清晰简单。容易上手，团队学习成本低，gateway有很多可以开箱即用的功能，非常方便。 gateway功能性更强，有非常多的predicate实现和filter，直接配置化就可以使用。同时还可以基于Redis记性流量控制。 "},"pages/SpringCloud/1266003506511216704.html":{"url":"pages/SpringCloud/1266003506511216704.html","title":"说说生产环境下,你们是怎么实现网关对服务的动态路由的?","keywords":"","body":"方案一: 数据库 如果映射关系写死，每次路由关系更改，就需要重启网关，影响会非常大，因此需要实现网关动态的更新路由关系。 可以使用第三方组件保存路由关系，然后在网关里面通过定时任务去定时刷新组件中保存的路由信息。 因此就可以基于mqsql去做路由关系的保存，然后通过后台管理系统去操作db，再由网关去定时查询db更新路由表，实现动态效果。Nginx（Kong、Nginx+Lua）：Nginx抗高并发的能力很强，少数几台机器部署一下，就可以抗很高的并发，精通Nginx源码，很难，c语言，很难说从Nginx内核层面去做一些二次开发和源码定制。 方案二: Config配置 将Spring Cloud Zuul的路由信息，配置在 Config Server 的env.yml中，将网关服务注册为Config Client，从Config Server获取路由信息。微服务架构的系统中，我们通常会使用轻量级的消息代理来构建一个共用的消息主题让系统中所有微服务实例都连接上来，由于该主题中产生的消息会被所有实例监听和消费，所以我们称它为消息总线。在总线上的各个实例都可以方便地广播一些需要让其他连接在该主题上的实例都知道的消息，例如配置信息的变更或者其他一些管理操作等。Bus就是Spring Cloud中的消息总线。 其他方案: Apollo, Redis... "},"pages/SpringCloud/1266003506809012225.html":{"url":"pages/SpringCloud/1266003506809012225.html","title":"如果网关需要抗每秒10万的高并发访问,你应该怎么对网关进行生产优化?","keywords":"","body":"方案 Zuul网关部署的是什么配置的机器，部署32核64G，对网关路由转发的请求，每秒抗个小几万请求是不成问题的，几台Zuul网关机器。每秒是1万请求，8核16G的机器部署Zuul网关，5台机器就够了。 "},"pages/SpringCloud/1266003506968395844.html":{"url":"pages/SpringCloud/1266003506968395844.html","title":"你们公司的网关是怎么技术选型的,假设有高并发场景?","keywords":"","body":"测试结果 实际压测经验，gateway包含鉴权、日志等业务操作。2C4G的机器两台。500TPS CPU才30% 左右。一般的系统，妥妥的没压力。如果需要更高并发。直接加机器配置即可。 "},"pages/SpringCloud/1266003507165528135.html":{"url":"pages/SpringCloud/1266003507165528135.html","title":"如果需要部署上万服务实例,现有的服务注册中心能否抗住?如何优化?","keywords":"","body":"服务注册中心结构图 核心思想 注册中心主从架构，分片存储服务注册表，服务按需主动拉取注册表，不用全量拉取/推送，避免反向通知瞬时高并发。 自研注意点 客户端 服务拉取: 不用全量拉取，按需拉取(有疑问，怎么设计按需拉取)。还需要一个用于校验拉取增量数据之后数据是否完整的过程。 心跳发送 服务下线 服务端 服务注册: 服务注册表注意读写高并发控制，保证线程安全，也要降低锁的争用。 健康检查: 单位时间内，如果所注册服务没有续约，则要将其下线。 集群同步: 根据具体业务需求，制定合适的集群架构方案，保证吞吐量。 "},"pages/SpringCloud/1266003507534626867.html":{"url":"pages/SpringCloud/1266003507534626867.html","title":"你们是如何基于网关实现灰度发布的?说说你们的灰度发布方案?","keywords":"","body":"定义 通过网关(Zuul, ZuulFilter)的发布开关，把少量流量导入到一两台部署了新版本的服务器上，进行测试，这个就叫灰度发布。 开发流程 准备一个数据库和一个表（也可以用Apollo配置中心、Redis、ZooKeeper，其实都可以），放一个灰度发布启用表，存入具体uri以及是否灰度发布的一些信息，然后搞一张映射表。 启动1个线程每隔多少时间就去刷新灰度发布表的数据写到ConcurrentHashMap里面。 接着搞一个filter继承ZuulFilter，重写里面几个函数，其中shouldFilter根据返回值去判断执不执行run。 因此再should里面遍历map去看这次请求是否有开启灰度发布，如果有就执行run里面的逻辑，就自定义一些分发逻辑，这里用的时通过version去判断和分发。 发布流程 首先通过后台系统更改灰度发布标识，后台线程更新了map后，就会去执行根据version分发的策略，将少部分流量分发到new的服务上，然后监控和对应的后台，如果没问题，就更改为current，全线上线，最后将灰度发布表示改回来。 "},"pages/SpringCloud/1266003507681427533.html":{"url":"pages/SpringCloud/1266003507681427533.html","title":"说说你们一个服务从开发到上线,服务注册 网关路由 服务调用的流程?","keywords":"","body":"调用流程 开发了一个新的服务，线上部署，配合网关动态路由的功能，在网关里配置一下路径和新服务的映射关系，此时请求过来直接就可以走到新的服务里去。对已有服务进行迭代和开发，新版本，灰度发布，新版本部署少数几台机器，通过一个界面，开启这个服务的灰度发布，此时zuul filter启用，按照你的规则，把少量的流量打入到新版本部署的机器上去。观察一下少量流量在新版本的机器上运行是否正常。版本改成current，全量机器部署，关闭灰度发布功能，网关就会把流量均匀分发给那个服务了。 "},"pages/SpringCloud/1266003507773702201.html":{"url":"pages/SpringCloud/1266003507773702201.html","title":"画一下你们系统架构的整体架构图?说说各个服务在生产环境怎么部署的?","keywords":"","body":"核心 服务框架,注册中心,网关 部署 中小型系统，拆分10-20个微服务。大型互联网公司，一般几百个，几千个微服务。 中小型，一般2-3台机器足够，把服务上线，服务发现优化到极致。 服务上线：注册表多级缓存同步至1秒，拉取频率降低至1秒。 服务心跳：1秒报1次。 故障发现：1秒检查一次，2，3秒没有认为没有故障等。 服务注册中心没有压力，服务注册中心部署2台机器，每台4C8G，高可用，每秒轻松几百请求，甚至上千。前提是数据库SQL别写的太烂。 网关机器配置稍微高一些，4C8G，一台扛每秒几百个请求，部署3~4台，保证每台网关机器压力较小，进一步保证可靠性。 "}}